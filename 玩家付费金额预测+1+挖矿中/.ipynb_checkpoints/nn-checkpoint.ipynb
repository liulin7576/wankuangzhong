{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv('../input/tap_fun_train.csv')\n",
    "test_df = pd.read_csv('../input/tap_fun_test.csv')\n",
    "test_df[\"prediction_pay_price\"] = -1\n",
    "test_usid = test_df.user_id\n",
    "data = pd.concat([train_df, test_df])\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "# We're going to be calculating memory usage a lot,\n",
    "# so we'll create a function to save us some time!\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "gl_int = data.select_dtypes(include=['int'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "gl_float = data.select_dtypes(include=['float'])\n",
    "col = data.select_dtypes(include=['int']).columns\n",
    "data.loc[:,col] = gl_float.apply(pd.to_numeric,downcast='float')\n",
    "del gl_int, gl_float\n",
    "gc.collect()\n",
    "train_df = data[data.prediction_pay_price != -1]\n",
    "test_df = data[data.prediction_pay_price == -1]\n",
    "test_usid = test_df.user_id\n",
    "\n",
    "#挑选训练的模型\n",
    "a = train_df.drop(columns=['register_time', 'prediction_pay_price', 'avg_online_minutes'])\n",
    "b = (a.values != 0).sum(axis=1) == 1\n",
    "drop_train_usid = a.iloc[b].user_id\n",
    "print(drop_train_usid.shape)\n",
    "\n",
    "ret = list(set(train_df.user_id).difference(set(train_df.loc[train_df.user_id.isin(drop_train_usid)].user_id)))\n",
    "print('删除只有3个值的用户后，其中付钱的人数占比为：', (train_df[train_df.user_id.isin(drop_train_usid)].prediction_pay_price != 0).sum() / len(drop_train_usid))\n",
    "print('删除前train_df',train_df.shape)\n",
    "train_df = train_df.loc[train_df.user_id.isin(ret)].copy()\n",
    "print('删除后train_df',train_df.shape)\n",
    "a = test_df.drop(columns=['register_time','prediction_pay_price', 'avg_online_minutes'])\n",
    "b = (a.values != 0).sum(axis=1) == 1\n",
    "drop_test_usid = a.iloc[b].user_id\n",
    "print(drop_test_usid.shape)\n",
    "# test_usid = test_df.user_id  #记录test中的这些user_id\n",
    "print('删除前test_df',test_df.shape)\n",
    "ret = list(set(test_df.user_id).difference(set(test_df.loc[test_df.user_id.isin(drop_test_usid)].user_id)))\n",
    "test_df = test_df.loc[test_df.user_id.isin(ret)].copy()\n",
    "print('删除后test_df',test_df.shape)\n",
    "\n",
    "\n",
    "# #看看有没有重复行\n",
    "train_duplicate_usid = train_df.loc[train_df.drop(columns=['user_id','register_time','prediction_pay_price']).duplicated()].user_id\n",
    "ret = list(set(train_df.user_id).difference(set(train_duplicate_usid)))\n",
    "print('删除前train_df',train_df.shape)\n",
    "print('删除的用户后，其中付钱的人数占比为：', (train_df[train_df.user_id.isin(train_duplicate_usid)].prediction_pay_price != 0).sum() / len(train_duplicate_usid))\n",
    "train_df = train_df.loc[train_df.user_id.isin(ret)].copy()\n",
    "print('删除后train_df',train_df.shape)\n",
    "duplicate_test_usid = test_df[test_df.drop(columns=['user_id','register_time','prediction_pay_price']).duplicated()].user_id\n",
    "print('删除前test_df',test_df.shape)\n",
    "ret = list(set(test_df.user_id).difference(set(duplicate_test_usid)))\n",
    "test_df = test_df.loc[test_df.user_id.isin(ret)].copy()\n",
    "print('删除后test_df',test_df.shape)\n",
    "drop_test_usid = drop_test_usid.append(duplicate_test_usid)\n",
    "print(drop_test_usid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e35adaafbd675922020a0986b036d12aa21e976"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.loc[(train_df.prediction_pay_price > 0)]\n",
    "test_df = test_df.loc[(test_df.pay_price != 0)]\n",
    "data = pd.concat([train_df,test_df])\n",
    "def process(data):\n",
    "    data['register_hour'] = data['register_time'].map(lambda x : int(x[11:13]))\n",
    "    data['register_time_day'] = data['register_time'].map(lambda x : x[5:10])\n",
    "\n",
    "    data.loc[:,'is_pay_price45'] = data['pay_price'].map(lambda x: 1 if x>0 else 0)\n",
    "    data.loc[:,'is_pay_099'] = data['pay_price'].map(lambda x: 1 if x<1 else 0)\n",
    "\n",
    "    have_pay_price_mean = data.groupby(['register_time_day'])['is_pay_price45'].mean()\n",
    "    have_pay_099_mean = data.groupby(['register_time_day'])['is_pay_099'].mean()\n",
    "    pay_099_ration = data.loc[data['is_pay_price45']>0,:].copy()\n",
    "    pay_099_ration = pay_099_ration.groupby(['register_time_day'])['is_pay_099'].mean()\n",
    "    data['have_pay_price_mean_hour'] = data['register_hour'].map(lambda x : have_pay_price_mean[x])\n",
    "    data['have_pay_099_mean_hour'] = data['register_hour'].map(lambda x : have_pay_099_mean[x])\n",
    "    data['pay_099_ration_hour'] = data['register_hour'].map(lambda x : pay_099_ration[x])\n",
    "    del data['register_hour']\n",
    "    del data['register_time_day']\n",
    "    \n",
    "    import time\n",
    "    a = data['register_time'].apply(lambda x:time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    a /= (3600 * 24)  #这里试试天数会不会更有效果\n",
    "    data['regedit_diff_day'] = (a - min(a))\n",
    "    \n",
    "     #这个特征是顾客在3月双休日点击APP的总次数\n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-02-02', '2018-02-09', '2018-02-16', '2018-02-23', '2018-03-02','2018-03-09','2018-03-16']\n",
    "    new['date_week'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_week', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    new = data[['prediction_pay_price','user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-02-13', '2018-03-16']\n",
    "    new['date_week_two'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_week_two', 'user_id']], on='user_id',how='left')\n",
    "\n",
    "    \n",
    "    #这个特征是顾客在3月双休日点击APP的总次数\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[0])\n",
    "#     new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "    week = ['2018-03-10', '2018-02-19']\n",
    "    new['date_holiday'] = new.date.apply(lambda x:1 if x in week else 0)\n",
    "    data = pd.merge(data, new[['date_holiday', 'user_id']], on='user_id',how='left')\n",
    "    \n",
    "    #这个特征是顾客在3月双休日点击APP的总次数\n",
    "    new = data[['user_id', 'register_time']]\n",
    "    new['date'] = new.register_time.apply(lambda x:x.split()[1])\n",
    "    new['date'] = new['date'].apply(lambda x:int(x.split(':')[0]))\n",
    "    new['date_h_1'] = new.date.apply(lambda x:1 if ((x >= 0) & (x < 4) )else 0)\n",
    "    new['date_h_2'] = new.date.apply(lambda x:1 if ((x >= 4) & (x < 8) )else 0)\n",
    "    new['date_h_3'] = new.date.apply(lambda x:1 if ((x >= 8) & (x < 12) )else 0)\n",
    "    new['date_h_4'] = new.date.apply(lambda x:1 if ((x >= 12) & (x < 16) )else 0)\n",
    "    new['date_h_5'] = new.date.apply(lambda x:1 if ((x >= 16) & (x < 20) )else 0)\n",
    "    new['date_h_6'] = new.date.apply(lambda x:1 if ((x >= 20) & (x < 24) )else 0)\n",
    "    data = pd.merge(data, new[['date_h_2','date_h_3','user_id']], on='user_id',how='left')\n",
    "    \n",
    "    data['register_time'] = pd.to_datetime(data['register_time'])\n",
    "    data['dow'] = data['register_time'].apply(lambda x:x.dayofweek)\n",
    "    data['doy'] = data['register_time'].apply(lambda x:x.dayofyear)\n",
    "#     data['day'] = data['register_time'].apply(lambda x:x.day)\n",
    "    data['month'] = data['register_time'].apply(lambda x:x.month)\n",
    "    data['hour'] = data['register_time'].apply(lambda x:x.hour)\n",
    "    data['minute'] = data['register_time'].apply(lambda x:x.hour*60 + x.minute)\n",
    "    for i in ['dow', 'doy', 'month']:\n",
    "        a = pd.get_dummies(data[i], prefix = i)\n",
    "        data = pd.concat([data, a], axis = 1)\n",
    "        del data[i]\n",
    "        \n",
    "    def get_poly_fea(data):\n",
    "        fea_list = ['ivory_add_value', 'wood_add_value', 'stone_add_value', 'general_acceleration_add_value', 'ivory_reduce_value', 'meat_add_value', 'wood_reduce_value', \n",
    "                'training_acceleration_add_value']\n",
    "        for i in fea_list:\n",
    "    #             data[i + str(i) + 'square'] = data[i] ** 2\n",
    "    #             data[i + str(i) + 'cubic'] = data[i] ** 3\n",
    "            data[i + str(i) + 'sqrt'] = data[i] ** 0.5\n",
    "        return data\n",
    "\n",
    "    data = get_poly_fea(data)\n",
    "\n",
    "    del data['register_time']\n",
    "    \n",
    "    data.loc[:,'wood_reduce_ratio'] = data['wood_reduce_value'] / (data['wood_add_value']+1e-4)\n",
    "    data.loc[:,'stone_reduce_ratio'] = data['stone_reduce_value'] / (data['stone_add_value']+1e-4)\n",
    "    data.loc[:,'ivory_reduce_ratio'] = data['ivory_reduce_value'] / (data['ivory_add_value']+1e-4)\n",
    "    data.loc[:,'meat_reduce_ratio'] = data['meat_reduce_value'] / (data['meat_add_value']+1e-4)\n",
    "    data.loc[:,'magic_reduce_ratio'] = data['magic_reduce_value'] / (data['magic_add_value']+1e-4)\n",
    "    data.loc[:,'infantry_reduce_ratio'] = data['infantry_reduce_value'] / (data['infantry_add_value']+1e-4)\n",
    "    data.loc[:,'cavalry_reduce_ratio'] = data['cavalry_reduce_value'] / (data['cavalry_add_value']+1e-4)\n",
    "    data.loc[:,'shaman_reduce_ratio'] = data['shaman_reduce_value'] / (data['shaman_add_value']+1e-4)\n",
    "    data.loc[:,'wound_infantry_reduce_ratio'] = data['wound_infantry_reduce_value'] / (data['wound_infantry_add_value']+1e-4)\n",
    "    data.loc[:,'wound_cavalry_reduce_ratio'] = data['wound_cavalry_reduce_value'] / (data['wound_cavalry_add_value']+1e-4)\n",
    "    data.loc[:,'wound_shaman_reduce_ratio'] = data['wound_shaman_reduce_value'] / (data['wound_shaman_add_value']+1e-4)\n",
    "    data.loc[:,'general_acceleration_reduce_ratio'] = data['general_acceleration_reduce_value'] / (data['general_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'building_acceleration_reduce_ratio'] = data['building_acceleration_reduce_value'] / (data['building_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'reaserch_acceleration_reduce_ratio'] = data['reaserch_acceleration_reduce_value'] / (data['reaserch_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'training_acceleration_reduce_ratio'] = data['training_acceleration_reduce_value'] / (data['training_acceleration_add_value']+1e-4)\n",
    "    data.loc[:,'treatment_acceleraion_reduce_ratio'] = data['treatment_acceleration_reduce_value'] / (data['treatment_acceleraion_add_value']+1e-4)\n",
    "    data.loc[:,'wood_add_sub_reduce'] = np.abs(data['wood_add_value'] - data['wood_reduce_value'])\n",
    "    data.loc[:,'stone_add_sub_reduce'] = np.abs(data['stone_add_value'] - data['stone_reduce_value'])\n",
    "    data.loc[:,'ivory_add_sub_reduce'] = np.abs(data['ivory_add_value'] - data['ivory_reduce_value'])\n",
    "    data.loc[:,'meat_add_sub_reduce'] = np.abs(data['meat_add_value'] - data['meat_reduce_value'])\n",
    "    data.loc[:,'magic_add_sub_reduce'] = np.abs(data['magic_add_value'] - data['magic_reduce_value'])\n",
    "    data.loc[:,'infantry_add_sub_reduce'] = np.abs(data['infantry_add_value'] - data['infantry_reduce_value'])\n",
    "    data.loc[:,'cavalry_add_sub_reduce'] = np.abs(data['cavalry_add_value'] - data['cavalry_reduce_value'])\n",
    "    data.loc[:,'shaman_add_sub_reduce'] = np.abs(data['shaman_add_value'] - data['shaman_reduce_value'])\n",
    "    data.loc[:,'wound_infantry_add_sub_reduce'] = np.abs(data['wound_infantry_add_value'] - data['wound_infantry_reduce_value'])\n",
    "    data.loc[:,'wound_cavalry_add_sub_reduce'] = np.abs(data['wound_cavalry_add_value'] - data['wound_cavalry_reduce_value'])\n",
    "    data.loc[:,'wound_shaman_add_sub_reduce'] = np.abs(data['wound_shaman_add_value'] - data['wound_shaman_reduce_value'])\n",
    "    data.loc[:,'general_acceleration_add_sub_reduce'] = np.abs(data['general_acceleration_add_value'] - data['general_acceleration_reduce_value'])\n",
    "    data.loc[:,'building_acceleration_add_sub_reduce'] = np.abs(data['building_acceleration_add_value'] - data['building_acceleration_reduce_value'])\n",
    "    data.loc[:,'reaserch_acceleration_add_sub_reduce'] = np.abs(data['reaserch_acceleration_add_value'] - data['reaserch_acceleration_reduce_value'])\n",
    "    data.loc[:,'training_acceleration_add_sub_reduce'] = np.abs(data['training_acceleration_add_value'] - data['training_acceleration_reduce_value'])\n",
    "    data.loc[:,'treatment_acceleration_add_sub_reduce'] = np.abs(data['treatment_acceleraion_add_value'] - data['treatment_acceleration_reduce_value'])\n",
    "    log_col = ['wood_add_value','wood_reduce_value','stone_add_value','stone_reduce_value','ivory_add_value',\n",
    "                'ivory_reduce_value','meat_add_value','meat_reduce_value','magic_add_value','magic_reduce_value',\n",
    "                'infantry_add_value','infantry_reduce_value','cavalry_add_value','cavalry_reduce_value','shaman_add_value',\n",
    "                'shaman_reduce_value','wound_infantry_add_value','wound_infantry_reduce_value','wound_cavalry_add_value',\n",
    "                'wound_cavalry_reduce_value','wound_shaman_add_value','wound_shaman_reduce_value',\n",
    "                'general_acceleration_add_value','general_acceleration_reduce_value','building_acceleration_add_value',\n",
    "                'building_acceleration_reduce_value','reaserch_acceleration_add_value','reaserch_acceleration_reduce_value',\n",
    "                'training_acceleration_add_value','training_acceleration_reduce_value','treatment_acceleraion_add_value',\n",
    "                'treatment_acceleration_reduce_value']\n",
    "    for col in log_col:\n",
    "        data[col] = data[col].map(lambda x : np.log1p(x))\n",
    "    # 物资消耗统计\n",
    "    ratio_col = ['wood_reduce_ratio','stone_reduce_ratio','ivory_reduce_ratio','meat_reduce_ratio','magic_reduce_ratio',\\\n",
    "                'infantry_reduce_ratio','cavalry_reduce_ratio','shaman_reduce_ratio','wound_infantry_reduce_ratio',\\\n",
    "                'wound_cavalry_reduce_ratio','wound_shaman_reduce_ratio','general_acceleration_reduce_ratio',\\\n",
    "                'building_acceleration_reduce_ratio','reaserch_acceleration_reduce_ratio','training_acceleration_reduce_ratio',\\\n",
    "                'treatment_acceleraion_reduce_ratio']\n",
    "    data.loc[:,'ratio_max'] = data[ratio_col].max(1)\n",
    "    data.loc[:,'ratio_min'] = data[ratio_col].min(1)\n",
    "    data.loc[:,'ratio_mean'] = data[ratio_col].mean(1)\n",
    "    data.loc[:,'ratio_sum'] = data[ratio_col].sum(1)\n",
    "    data.loc[:,'ratio_std'] = data[ratio_col].std(1)\n",
    "    data.loc[:,'ratio_median'] = data[ratio_col].median(1)\n",
    "    # data.loc[:,'ratio_mode'] = data[ratio_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "    # 物资生产统计\n",
    "    add_col = ['wood_add_value','stone_add_value','ivory_add_value','meat_add_value','magic_add_value',\\\n",
    "                'infantry_add_value','cavalry_add_value','shaman_add_value','wound_infantry_add_value',\\\n",
    "                'wound_cavalry_add_value','wound_shaman_add_value','general_acceleration_add_value',\\\n",
    "                'building_acceleration_add_value','reaserch_acceleration_add_value','training_acceleration_add_value',\\\n",
    "                'treatment_acceleraion_add_value']\n",
    "    data.loc[:,'add_max'] = data[add_col].max(1)\n",
    "    data.loc[:,'add_min'] = data[add_col].min(1)\n",
    "    data.loc[:,'add_mean'] = data[add_col].mean(1)\n",
    "    data.loc[:,'add_sum'] = data[add_col].sum(1)\n",
    "    data.loc[:,'add_std'] = data[add_col].std(1)\n",
    "    data.loc[:,'add_median'] = data[add_col].median(1)\n",
    "    # data.loc[:,'add_mode'] = data[add_col].mode(1)\n",
    "    \n",
    "    reduce_col = ['wood_reduce_value','stone_reduce_value','ivory_reduce_value','meat_reduce_value','magic_reduce_value',\\\n",
    "                'infantry_reduce_value','cavalry_reduce_value','shaman_reduce_value','wound_infantry_reduce_value',\\\n",
    "                'wound_cavalry_reduce_value','wound_shaman_reduce_value','general_acceleration_add_value',\\\n",
    "                'building_acceleration_reduce_value','reaserch_acceleration_reduce_value','training_acceleration_reduce_value',\\\n",
    "                'treatment_acceleration_reduce_value']\n",
    "    data.loc[:,'reduce_max'] = data[reduce_col].max(1)\n",
    "    data.loc[:,'reduce_min'] = data[reduce_col].min(1)\n",
    "    data.loc[:,'reduce_mean'] = data[reduce_col].mean(1)\n",
    "    data.loc[:,'reduce_sum'] = data[reduce_col].sum(1)\n",
    "    data.loc[:,'reduce_std'] = data[reduce_col].std(1)\n",
    "    data.loc[:,'reduce_median'] = data[reduce_col].median(1)\n",
    "    # data.loc[:,'reduce_mode'] = data[reduce_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "    # 建筑等级统计\n",
    "    bd_col = ['bd_training_hut_level','bd_healing_lodge_level','bd_stronghold_level','bd_outpost_portal_level',\n",
    "            'bd_barrack_level','bd_healing_spring_level','bd_dolmen_level','bd_guest_cavern_level','bd_warehouse_level',\n",
    "            'bd_watchtower_level','bd_magic_coin_tree_level','bd_hall_of_war_level','bd_market_level','bd_hero_gacha_level',\n",
    "            'bd_hero_strengthen_level','bd_hero_pve_level']\n",
    "    data.loc[:,'bd_max'] = data[bd_col].max(1)\n",
    "    data.loc[:,'bd_min'] = data[bd_col].min(1)\n",
    "    data.loc[:,'bd_mean'] = data[bd_col].mean(1)\n",
    "    data.loc[:,'bd_sum'] = data[bd_col].sum(1)\n",
    "    data.loc[:,'bd_std'] = data[bd_col].std(1)\n",
    "    data.loc[:,'bd_median'] = data[bd_col].median(1)\n",
    "    # data.loc[:,'bd_mode'] = data[bd_col].mode(1)\n",
    "    \n",
    "    # 科研 tier 统计\n",
    "    tier_col = ['sr_infantry_tier_2_level','sr_cavalry_tier_2_level','sr_shaman_tier_2_level',\n",
    "                'sr_infantry_tier_3_level','sr_cavalry_tier_3_level','sr_shaman_tier_3_level',\n",
    "                'sr_infantry_tier_4_level','sr_cavalry_tier_4_level','sr_shaman_tier_4_level']\n",
    "\n",
    "    data.loc[:,'infantry_tier_sum'] = data[[tier_col[0],tier_col[3],tier_col[6]]].sum(1)\n",
    "    data.loc[:,'cavalry_tier_sum'] = data[[tier_col[1],tier_col[4],tier_col[7]]].sum(1)\n",
    "    data.loc[:,'shaman_tier_sum'] = data[[tier_col[2],tier_col[5],tier_col[8]]].sum(1)\n",
    "\n",
    "\n",
    "    data.loc[:,'sr_tier_sum'] = data[tier_col].sum(1)\n",
    "    data.loc[:,'sr_tier_max'] = data[tier_col].max(1)\n",
    "    data.loc[:,'sr_tier_min'] = data[tier_col].min(1)\n",
    "    data.loc[:,'sr_tier_mean'] = data[tier_col].mean(1)\n",
    "    data.loc[:,'sr_tier_std'] = data[tier_col].std(1)\n",
    "    data.loc[:,'sr_tier_median'] = data[tier_col].median(1)\n",
    "    # data.loc[:,'sr_tier_mode'] = data[tier_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "    # 攻击\n",
    "    atk_col = ['sr_infantry_atk_level','sr_cavalry_atk_level','sr_shaman_atk_level','sr_troop_attack_level']\n",
    "    data.loc[:,'atk_sum'] = data[atk_col].sum(1)\n",
    "    data.loc[:,'atk_mean'] = data[atk_col].mean(1)\n",
    "    data.loc[:,'atk_max'] = data[atk_col].max(1)\n",
    "    data.loc[:,'atk_std'] = data[atk_col].std(1)\n",
    "    data.loc[:,'atk_min'] = data[atk_col].min(1)\n",
    "    data.loc[:,'atk_median'] = data[atk_col].median(1)\n",
    "    # data.loc[:,'atk_mode'] = data[atk_col].mode(1)\n",
    "    \n",
    "     # 防御\n",
    "    def_col = ['sr_infantry_def_level','sr_cavalry_def_level','sr_shaman_def_level','sr_troop_defense_level']\n",
    "    data.loc[:,'def_sum'] = data[def_col].sum(1)\n",
    "    data.loc[:,'def_mean'] = data[def_col].mean(1)\n",
    "    data.loc[:,'def_max'] = data[def_col].max(1)\n",
    "    data.loc[:,'def_min'] = data[def_col].min(1)\n",
    "    data.loc[:,'def_std'] = data[def_col].std(1)\n",
    "    # data.loc[:,'def_mode'] = data[def_col].mode(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 生命力\n",
    "    hp_col = ['sr_infantry_hp_level','sr_cavalry_hp_level','sr_shaman_hp_level']\n",
    "    data.loc[:,'hp_sum'] = data[hp_col].sum(1)\n",
    "    data.loc[:,'hp_mean'] = data[hp_col].mean(1)\n",
    "    data.loc[:,'hp_max'] = data[hp_col].max(1)\n",
    "    # data.loc[:,'hp_mode'] = data[hp_col].mode(1)\n",
    "    \n",
    "    # 各种 level 统计\n",
    "    level_col = ['sr_construction_speed_level','sr_hide_storage_level','sr_troop_consumption_level',\n",
    "                'sr_rss_a_prod_levell','sr_rss_b_prod_level','sr_rss_c_prod_level',\n",
    "                'sr_rss_d_prod_level','sr_rss_a_gather_level','sr_rss_b_gather_level',\n",
    "                'sr_rss_c_gather_level','sr_rss_d_gather_level','sr_troop_load_level','sr_rss_e_gather_level',\n",
    "                'sr_rss_e_prod_level','sr_outpost_durability_level','sr_outpost_tier_2_level',\n",
    "                'sr_healing_space_level','sr_gathering_hunter_buff_level','sr_healing_speed_level',\n",
    "                'sr_outpost_tier_3_level','sr_alliance_march_speed_level','sr_pvp_march_speed_level',\n",
    "                'sr_gathering_march_speed_level','sr_outpost_tier_4_level','sr_guest_troop_capacity_level',\n",
    "                'sr_march_size_level','sr_rss_help_bonus_level',]\n",
    "    data.loc[:,'same_level_sum'] = data[level_col].sum(1)\n",
    "    data.loc[:,'same_level_mean'] = data[level_col].mean(1)\n",
    "    data.loc[:,'same_level_max'] = data[level_col].max(1)\n",
    "    data.loc[:,'same_level_std'] = data[level_col].std(1)\n",
    "    data.loc[:,'same_level_min'] = data[level_col].min(1)\n",
    "    data.loc[:,'same_level_median'] = data[level_col].median(1)\n",
    "    # data.loc[:,'same_level_mode'] = data[level_col].mode(1)\n",
    "    \n",
    "     # pvp\n",
    "\n",
    "    data.loc[:,'pvp_lanch_ratio'] = data['pvp_lanch_count'] / (data['pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pvp_win_ratio'] = data['pvp_win_count'] / (data['pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pvp_win_lanch_ratio'] = data['pvp_win_count'] / (data['pvp_lanch_count'] + 1e-4)\n",
    "\n",
    "    # pve\n",
    "    data.loc[:,'pve_lanch_ratio'] = data['pve_lanch_count'] / (data['pve_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_win_ratio'] = data['pve_win_count'] / (data['pve_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_win_lanch_ratio'] = data['pve_win_count'] / (data['pve_lanch_count'] + 1e-4)\n",
    "\n",
    "    data.loc[:,'pve_pvp_battle_count'] = data['pvp_battle_count'] + data['pve_battle_count']\n",
    "    data.loc[:,'pve_pvp_lanch_count'] = data['pvp_lanch_count'] + data['pve_lanch_count']\n",
    "    data.loc[:,'pve_pvp_win_count'] = data['pvp_win_count'] + data['pve_win_count']\n",
    "\n",
    "    data.loc[:,'pve_pvp_lanch_'] = data['pve_pvp_lanch_count'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_pvp_win_ratio'] = data['pve_pvp_win_count'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'pve_pvp_win_lanch_ratio'] = data['pve_pvp_win_count'] / (data['pve_pvp_lanch_count'] + 1e-4)\n",
    "\n",
    "    # 时间、消费\n",
    "    data.loc[:,'pay_mean_count'] = data['pay_price'] / (data['pay_count'] + 1e-4)\n",
    "    data.loc[:,'pay_mean_online_minutes'] = data['pay_price'] / (data['avg_online_minutes'] + 1e-4)\n",
    "    data.loc[:,'pay_count_online_minutes'] = data['avg_online_minutes'] / (data['pay_count'] + 1e-4)\n",
    "\n",
    "    data.loc[:,'time_lanch_per'] = data['avg_online_minutes'] / (data['pve_pvp_lanch_count'] + 1e-4)\n",
    "    data.loc[:,'money_lanch_per'] = data['pay_price'] / (data['pve_pvp_lanch_count'] + 1e-4)\n",
    "    \n",
    "    data.loc[:,'time_battle_per'] = data['avg_online_minutes'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "    data.loc[:,'money_battle_per'] = data['pay_price'] / (data['pve_pvp_battle_count'] + 1e-4)\n",
    "\n",
    "    # data.loc[:,'mean_pay_price'] = data['pay_price'] / 7\n",
    "\n",
    "    data.loc[:,'time_win_per'] = data['avg_online_minutes'] / (data['pve_pvp_win_count'] + 1e-4)\n",
    "    data.loc[:,'money_win_per'] = data['pay_price'] / (data['pve_pvp_win_count'] + 1e-4)\n",
    "    data.loc[:,'pay_count_win_per'] = data['pay_count'] / (data['pve_pvp_win_count'] + 1e-4)\n",
    "    return data\n",
    "\t\n",
    "data = process(data)\n",
    "\n",
    "\n",
    "def split(data):\n",
    "    train = data[data.prediction_pay_price != -1]\n",
    "    test = data[data.prediction_pay_price == -1]  \n",
    "    test_usid = test.user_id\n",
    "    numeric_feats = data.drop(columns=['prediction_pay_price', 'user_id']).dtypes[data.drop(columns=['prediction_pay_price', 'user_id']).dtypes != \"object\"].index\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "    print(skewed_feats.shape)\n",
    "    skewed_feats = skewed_feats.index\n",
    "    train[skewed_feats] = np.log1p(train[skewed_feats])\n",
    "    test[skewed_feats] = np.log1p(test[skewed_feats])\n",
    "    del test['user_id']\n",
    "    del test['prediction_pay_price']\n",
    "    test_X = test.values.astype(np.float32)\n",
    "    del train['user_id']   \n",
    "    train = train.loc[(train['prediction_pay_price']<16000)]  \n",
    "    y = train['prediction_pay_price'].values.astype(np.float32)\n",
    "    del train['prediction_pay_price']\n",
    "    X = train.values.astype(np.float32)\n",
    "    col = train.columns\n",
    "    return X, y, test_X, train, test, col, test_usid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28091e0cd21bb2f4a66481d115b872fc8d27e750"
   },
   "outputs": [],
   "source": [
    "X, y, test_X, train, test, col, test_with_pay_usid  = split(data)\n",
    "y = np.log1p(y)\n",
    "mms = MinMaxScaler()\n",
    "X = mms.fit_transform(X)\n",
    "test_X = mms.transform(test_X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=42)\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout,Activation\n",
    "from keras.layers.normalization import  BatchNormalization  as bn\n",
    "from  keras.layers.pooling import MaxPooling1D as pool\n",
    "\n",
    "# early stoppping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='mse', patience=50, verbose=0)\n",
    "# 训练\n",
    "# history = model.fit(train_X, train_y, epochs=300, batch_size=20, validation_data=(test_X, test_y), verbose=2, shuffle=False, )\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    # This returns a tensor\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "    import  keras.utils \n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    x = Dense(180)(inputs)\n",
    "#     x = bn()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "\n",
    "    x = Dense(100)(x)\n",
    "#     x = bn()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    predictions = Dense(1, activation='linear')(x)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    model = Model(input=inputs, output=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mean_squared_error',\n",
    "                 metrics=['mse'])\n",
    "    return model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd8be153b8e293b8bb64d4bd06681ea73b9a1bb5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rmsel(y_true,y_pre):\n",
    "    return mean_squared_error(y_true,y_pre)**0.5\n",
    "\n",
    "kf = KFold(n_splits=10,random_state=24,shuffle=True)\n",
    "best_rmse = []\n",
    "pred_list = []\n",
    "\n",
    "pred_train = []\n",
    "pred_train_list = []\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_val = X[val_index]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    # regr = LinearRegression()\n",
    "#     regr = Ridge(alpha=1.0, max_iter=100, tol=0.001, random_state=24)\n",
    "#     regr = RandomForestRegressor(n_estimators=120,max_depth=8, random_state=0)\n",
    "#     regr = GradientBoostingRegressor(n_estimators=100, subsample=0.9)\n",
    "    regr = baseline_model()\n",
    "#     regr.fit(X_train,y_train)\n",
    "    regr.fit(X_train, y_train, nb_epoch=25, batch_size=256, verbose = 1)\n",
    "    pred_train = (regr.predict(X))\n",
    "#     pred_train = np.where(pred_train<1,0.99,pred_train)\n",
    "    pred_train_list.append(pred_train)\n",
    "    \n",
    "    predi = (regr.predict(X_val))\n",
    "#     predi = np.where(predi<1,0.99,predi)\n",
    "\n",
    "    # rmse = rmsel(np.expm1(y_val), np.expm1(predi))\n",
    "    rmse = rmsel((y_val), predi)\n",
    "\n",
    "    print(\"cv: \",rmse)\n",
    "\n",
    "    predi = (regr.predict(test_X))\n",
    "#     predi = np.where(predi<1,0.99,predi)\n",
    "\n",
    "    pred_list.append(predi)\n",
    "    best_rmse.append(rmse)\n",
    "\n",
    "pred = np.mean(np.array(pred_list),axis=0)\n",
    "meanrmse = np.mean(best_rmse)\n",
    "stdrmse = np.std(best_rmse)\n",
    "pred_train = np.mean(np.array(pred_train_list),axis=0)\n",
    "print('10 flod mean rmse, std rmse:',(meanrmse,stdrmse))\n",
    "\n",
    "test_with_pay = pd.DataFrame()\n",
    "test_with_pay['user_id'] = test_with_pay_usid\n",
    "test_with_pay['prediction_pay_price'] = np.expm1(pred)\n",
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = test_usid.values\n",
    "# sub['prediction_pay_price'] = preds\n",
    "sub['prediction_pay_price'] = 0\n",
    "sub.loc[sub.user_id.isin(test_with_pay.user_id), 'prediction_pay_price'] = test_with_pay['prediction_pay_price'].values\n",
    "print(sub.head(), '\\n')\n",
    "print(sub.describe())\n",
    "sub.to_csv('nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17cb59c5f13778367433a5ce0e812131e550c0db"
   },
   "outputs": [],
   "source": [
    "sub[sub.prediction_pay_price != 0].sort_values('prediction_pay_price', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6eb4bf9c5b4ea2378b5e9c6eaf787b8f7440f052"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
